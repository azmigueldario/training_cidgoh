{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9198e9a5-e26c-4ccb-bc4e-9fac160f96e8",
   "metadata": {},
   "source": [
    "# Annotation of genomes\n",
    "\n",
    "## Introduction to annotation tools\n",
    "\n",
    "Once we have analysis ready genomes (verified quality and lack of contamination), we can do annotation. The main goal of annotation is to identify the genomic features that are present in the contigs. There are many available tools to find these relevant sequences, and they mostly use predictive algorithms (recognition of common sequences in the DNA) or alignment against a reference database. \n",
    "\n",
    "Two of the most commonly used tools are [BAKTA](https://github.com/oschwengers/bakta)  (https://doi.org/10.1099/mgen.0.000685) and the [NCBI Prokaryotic Genome Annotation Pipeline](https://github.com/ncbi/pgap) (https://doi.org/10.1093/nar/gkaa1105). \n",
    "\n",
    "Bakta produces a comprehensive annotation of Open Reading Frames (sequences that code for amino acids over a length threshold), different types of RNA (ncRNA, tRNA, rRNA), hypothetical proteins, genes and CRISPR arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeaab644",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# source PATH to use module function\n",
    "source /cvmfs/soft.computecanada.ca/config/profile/bash.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab7b4637",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Bakta uses a reference database that is to large for a tutorial (~30GB), so we are using a reduced database (~3.0 GB) that may produce less reliable feature annotation. \n",
    "\n",
    "The database was downloaded and decompressed into `/tools/reference_data/db-light` using the command below\n",
    "```\n",
    "singularity exec -B /scratch,/project,/etc </PATH/TO/BAKTA/IMAGE/> \\\n",
    "    bakta_db download \\\n",
    "    --output </OUTPUT/DIR/> \\\n",
    "    --type light\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13029f3c",
   "metadata": {},
   "source": [
    "## Annotation of draft genomes\n",
    "\n",
    "First, we can review the structure of our working directory. \n",
    "\n",
    "### **Note:** Remember to have **bash_kernel** for the Jupyter notebook loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2d6c63",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-mdprieto/tutorials\n",
      "|-- results\n",
      "|   |-- annotation\n",
      "|   |-- assembly_checkm\n",
      "|   |-- assembly_quast\n",
      "|   |-- contigs\n",
      "|   |-- reads_qc\n",
      "|   `-- snippy\n",
      "`-- trimmed_reads\n",
      "\n",
      "8 directories\n",
      "/mnt/cidgoh-object-storage/seagull/jupyter-mdprieto/\n",
      "|-- baktadb-light\n",
      "|   `-- amrfinderplus-db\n",
      "|-- raw_reads\n",
      "`-- reference_data\n",
      "\n",
      "4 directories\n"
     ]
    }
   ],
   "source": [
    "# review the project structure\n",
    "tree -dL 2 ~/tutorials\n",
    "\n",
    "# software directory structure\n",
    "tree -dL 2 /mnt/cidgoh-object-storage/seagull/jupyter-mdprieto/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aba8980b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "We save the path to the **bakta singularity image** and the **reference database** in the following [ENVIRONMENTAL VARIABLES](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/) to use them in our commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235e8d53-d8cb-4289-8e41-c9f98d8e89bb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "BAKTA_IMG=\"/mnt/cidgoh-object-storage/images/bakta_1.7.sif\"\n",
    "BAKTA_DB=\"/mnt/cidgoh-object-storage/seagull/jupyter-mdprieto/baktadb-light\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219b0807",
   "metadata": {},
   "source": [
    "Now, we execute the command `bakta` specifying a single draft genome. Annotation for the remaining ones is already provided (7 min per annotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98acc0b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# remove previously produced results\n",
    "rm ~/tutorials/results/annotation/ERR10479518*\n",
    "\n",
    "# add drives so singularity can look inside them\n",
    "export SINGULARITY_BIND=\"/mnt,/etc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0309dd4-fa2b-4a4d-af80-145bc45e48a7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:\u001b[0m While bind mounting '/etc:/etc': destination is already in the mount point list\n",
      "\n",
      "parse genome sequences...\n",
      "\timported: 142\n",
      "\tfiltered & revised: 142\n",
      "\tcontigs: 142\n",
      "\n",
      "start annotation...\n",
      "predict tRNAs...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/bakta\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bakta/main.py\", line 153, in main\n",
      "    genome['features'][bc.FEATURE_T_RNA] = t_rna.predict_t_rnas(genome, contigs_path)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bakta/features/t_rna.py\", line 59, in predict_t_rnas\n",
      "    proc = sp.run(\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1154, in communicate\n",
      "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 2005, in _communicate\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "singularity exec -B /etc \"$BAKTA_IMG\" bakta \\\n",
    "    --db $BAKTA_DB                                                                  `# path to bakta database` \\\n",
    "    ~/tutorials/results/contigs/ERR10479518_contigs.fa                              `# file to annotate`\\\n",
    "    --output ~/tutorials/results/annotation/                                        `# output directory` \\\n",
    "    --genus Pseudomonas                                                             `# specify genus of isolate` \\\n",
    "    --prefix $(echo \"ERR10479518_contigs.fa\" | grep -Eo \"ERR[0-9]+\")                `# prefix of sample name only for output`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b54117",
   "metadata": {},
   "source": [
    "Now, our analysis results look something like this. The directory structure now includes results from annotation and the tools directory includes a reference database to run bakta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46cf61b4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-mdprieto/tutorials\n",
      "|-- results\n",
      "|   |-- annotation\n",
      "|   |-- assembly_checkm\n",
      "|   |-- assembly_quast\n",
      "|   |-- contigs\n",
      "|   |-- reads_qc\n",
      "|   `-- snippy\n",
      "`-- trimmed_reads\n",
      "\n",
      "8 directories\n"
     ]
    }
   ],
   "source": [
    "# project structure\n",
    "tree -dL 2 ~/tutorials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78971d04",
   "metadata": {},
   "source": [
    "The output of **Bakta** annotation can be found in `tutorials/results/annotation`. \n",
    "\n",
    "- Circular plots can be visualized directly on Jupyter by clicking them or downloaded to your local computer and opened.\n",
    "- Annotation results in this dataset are not ideal as we are working with draft genomes instead of complete chromosome asssemblies or scaffolds. These more refined assemblies require long reads to polish the contigs.\n",
    "\n",
    "Take a look at this paper for more information about assembly approaches: https://doi.org/10.1093/bib/bbw096. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "980567bf",
   "metadata": {},
   "source": [
    "# Phylogenetic tree\n",
    "\n",
    "To produce a phylogenetic tree we must compare the genomes using some kind of metric. Here, we will use **Snippy** to identify single nucleotide variants (SNV) in our genomes (https://github.com/tseemann/snippy). \n",
    "\n",
    "**Gubbins** is a tool that removes genes or features that were obtained through horizontal gene transfer and that may obscure the interpretation of the phylogenetic tree (https://github.com/nickjcroucher/gubbins). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53365b4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "SNIPPY_IMG=\"/mnt/cidgoh-object-storage/images/snippy_4.6.0.sif\"\n",
    "GUBBINS_IMG=\"/mnt/cidgoh-object-storage/images/gubbins-3.2.1.img\"\n",
    "REF_GENOME=\"/mnt/cidgoh-object-storage/seagull/jupyter-mdprieto/reference_data/GCF_000006765.1_ASM676v1_genomic.fna\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dafef90b",
   "metadata": {},
   "source": [
    "**Snippy** is a relatively fast tool, so we can analyze all the genomes we have available without too much wait. We apply the tool to every available draft genome. \n",
    "\n",
    "- The recommended use for the tool is actually on raw reads. \n",
    "\n",
    "To avoid typing the same command for every draft genome, we use a [**_for loop_**](https://ryanstutorials.net/bash-scripting-tutorial/bash-loops.php) to quickly reproduce the same command for a list of contig files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90d430aa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ERR10479510\n",
      "[M::bwa_idx_load_from_disk] read 0 ALT contigs\n",
      "[samclip] samclip 0.4.0 by Torsten Seemann (@torstenseemann)\n",
      "[samclip] Loading: reference/ref.fa.fai\n",
      "[samclip] Found 1 sequences in reference/ref.fa.fai\n",
      "[M::process] read 320000 sequences (80000000 bp)...\n",
      "[M::process] read 253774 sequences (63420526 bp)...\n",
      "[M::mem_process_seqs] Processed 320000 reads in 33.555 CPU sec, 4.139 real sec\n",
      "[samclip] Processed 100000 records...\n",
      "[samclip] Processed 200000 records...\n",
      "[samclip] Processed 300000 records...\n",
      "[M::mem_process_seqs] Processed 253774 reads in 28.675 CPU sec, 3.523 real sec\n",
      "[samclip] Processed 400000 records...\n",
      "[samclip] Processed 500000 records...\n",
      "[main] Version: 0.7.17-r1188\n",
      "[main] CMD: bwa mem[samclip] Total SAM records 574968, removed 9431, allowed 1610, passed 565537\n",
      " -Y[samclip] Header contained 3 lines\n",
      " -M[samclip] Done.\n",
      " -R @RG\\tID:ERR10479510\\tSM:ERR10479510 -t 8 reference/ref.fa fake_reads.fq\n",
      "[main] Real time: 9.468 sec; CPU: 63.437 sec\n",
      "[bam_sort_core] merging from 0 files and 3 in-memory blocks...\n",
      "[bam_sort_core] merging from 0 files and 3 in-memory blocks...\n"
     ]
    }
   ],
   "source": [
    "for CONTIG in $(ls $HOME/tutorials/results/contigs/*contigs.fa)\n",
    "do\n",
    "    \n",
    "    # define isolate name\n",
    "    SAMPLE_PREFIX=$(basename \"$CONTIG\" '_contigs.fa')\n",
    "    echo \"Processing $SAMPLE_PREFIX\"\n",
    "\n",
    "    # run snippy for each isolate\n",
    "    singularity exec $SNIPPY_IMG snippy \\\n",
    "    --outdir $HOME/tutorials/results/snippy/$SAMPLE_PREFIX  `# save in a subdirectory named after sample ID` \\\n",
    "    --ctgs \"$CONTIG\" \\\n",
    "    --ref $REF_GENOME \\\n",
    "    --cpus 8 \\\n",
    "    --force \\\n",
    "    --cleanup \\\n",
    "    --quiet\n",
    "\n",
    "done\n",
    "\n",
    "# produce core genome analysis in results_snippy folder\n",
    "\n",
    "cd $HOME/tutorials/results/snippy/\n",
    "singularity exec \"$SNIPPY_IMG\" snippy-core \\\n",
    "    --ref $REF_GENOME \\\n",
    "    $HOME/tutorials/results/snippy/ERR*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d10ecbf5",
   "metadata": {},
   "source": [
    "Once we have the data from the previous command, we perform a few additional steps to clean our results and have them ready for phylogenetic analysis. \n",
    "\n",
    "1. We run **snippy-core** to summarize the **SNVs** that are present only in genes contained in all isolates\n",
    "\n",
    "2. Then, **Gubbins** eliminates the features that are transmitted through horizontal gene transfer. \n",
    "\n",
    "3. Once we have the cleaned alignment, we use **snp-sites** to extract the sites in the alignment with a **SNV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d37a32",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mFATAL:  \u001b[0m could not open image /home/jupyter-mdprieto/tutorials/results/snippy/snippy-clean_full_aln: failed to retrieve path for /home/jupyter-mdprieto/tutorials/results/snippy/snippy-clean_full_aln: lstat /home/jupyter-mdprieto/tutorials/results/snippy/snippy-clean_full_aln: no such file or directory\n",
      "\u001b[31mFATAL:  \u001b[0m could not open image /home/jupyter-mdprieto/tutorials/results/snippy/run_gubbins.py: failed to retrieve path for /home/jupyter-mdprieto/tutorials/results/snippy/run_gubbins.py: lstat /home/jupyter-mdprieto/tutorials/results/snippy/run_gubbins.py: no such file or directory\n",
      "\u001b[31mFATAL:  \u001b[0m could not open image /home/jupyter-mdprieto/tutorials/results/snippy/snp-sites: failed to retrieve path for /home/jupyter-mdprieto/tutorials/results/snippy/snp-sites: lstat /home/jupyter-mdprieto/tutorials/results/snippy/snp-sites: no such file or directory\n"
     ]
    },
    {
     "ename": "",
     "evalue": "255",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "cd $HOME/tutorials/results/snippy\n",
    "\n",
    "# clean file\n",
    "singularity exec $SNIPPY_IMG snippy-clean_full_aln core.full.aln > clean.full.aln\n",
    "\n",
    "# gubbins remove horizontal gene transfer \n",
    "singularity exec $GUBBINS_IMG run_gubbins.py -p gubbins clean.full.aln\n",
    "\n",
    "# SNP-sites\n",
    "singularity exec $SNIPPY_IMG snp-sites -c gubbins.filtered_polymorphic_sites.fasta > clean.core.aln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82a4cb75",
   "metadata": {},
   "source": [
    "Once we have conducted all of the above steps. We have a file that contains the **SNVs** present in genes common to all isolates (core-genes) and that were not obtained by horizontal gene transfer. \n",
    "\n",
    "This file can be used to create a phylogenetic tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6949ee7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastTree Version 2.1.11 SSE3\n",
      "Alignment: clean.core.aln\n",
      "Nucleotide distances: Jukes-Cantor Joins: balanced Support: SH-like 1000\n",
      "Search: Normal +NNI +SPR (2 rounds range 10) +ML-NNI opt-each=1\n",
      "TopHits: 1.00*sqrtN close=default refresh=0.80\n",
      "ML Model: Generalized Time-Reversible, CAT approximation with 20 rate categories\n",
      "Initial topology in 0.05 seconds\n",
      "Refining topology: 14 rounds ME-NNIs, 2 rounds ME-SPRs, 7 rounds ML-NNIs\n",
      "Total branch-length 3.002 after 0.72 sec, 1 of 9 splits   \n",
      "ML-NNI round 1: LogLk = -72517.017 NNIs 1 max delta 3.17 Time 1.28\n",
      "GTR Frequencies: 0.2436 0.2580 0.2725 0.2259ep 12 of 12   \n",
      "GTR rates(ac ag at cg ct gt) 1.1798 14.4592 0.8504 0.7559 53.9537 1.0000\n",
      "Switched to using 20 rate categories (CAT approximation)15 of 20   \n",
      "Rate categories were divided by 0.677 so that average rate = 1.0\n",
      "CAT-based log-likelihoods may not be comparable across runs\n",
      "Use -gamma for approximate but comparable Gamma(20) log-likelihoods\n",
      "ML-NNI round 2: LogLk = -67208.532 NNIs 1 max delta 0.00 Time 5.51\n",
      "Turning off heuristics for final round of ML NNIs (converged)\n",
      "ML-NNI round 3: LogLk = -67208.533 NNIs 1 max delta 0.00 Time 5.99 (final)\n",
      "Optimize all lengths: LogLk = -67208.533 Time 6.23\n",
      "Total time: 7.32 seconds Unique: 11/11 Bad splits: 0/8\n"
     ]
    }
   ],
   "source": [
    "# produce tree\n",
    "$HOME/tutorials/tools/FastTree -gtr -nt clean.core.aln > clean.core.tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "866f3030",
   "metadata": {},
   "source": [
    "Finally, we can visualize the tree we just produce by pasting the contents of the file `clean.core.tree` in a visualizer tool such as [phyl.io](https://phylo.io/), or [iTOL](https://itol.embl.de/upload.cgi).\n",
    "\n",
    "The data we used may not be the best representation of differences in a phylogenetic tree as it comes from a single center outbreak and the samples may be highly similar among them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e18303e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
